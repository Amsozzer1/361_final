{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "## Exercise 1:\n",
    "\n",
    "\n",
    "### Setup\n",
    "$$ f(\\theta) = \\frac{1}{k} \\sum_{j=1}^k Q(\\theta , j) $$\n",
    "\n",
    "where $ Q(\\theta , j) $ is the loss function for the $ j^{th} $ data point.\n",
    " \n",
    "$ g(\\theta) = Q(\\theta , j) $ where $ i = \\{1,...,k\\} $\n",
    "\n",
    "As we know, \n",
    "$$ z = Q(\\theta , i) - f(\\theta) $$\n",
    "\n",
    "And\n",
    "$$ \\triangledown g(\\theta) = \\triangledown Q(\\theta , i) $$\n",
    "\n",
    "Hence,\n",
    "$$ g(\\theta) = f(\\theta) + z $$\n",
    "\n",
    "### Part 1\n",
    "\n",
    "$$ E [g(\\theta)] = E[Q(\\theta , i)] + E[Z] \\text{;  } E[Z] = 0$$\n",
    "$$ E [g(\\theta)] = \\frac{1}{k} \\sum_{i=1}^k Q(\\theta , i) + 0 $$\n",
    "$$ E [g(\\theta)] = f(\\theta) \\text{     Hence Proved  } $$\n",
    "\n",
    "### Part 2\n",
    "\n",
    "$$ \\triangledown f(\\theta) = \\triangledown (\\frac{1}{k} \\sum_{j=1}^k Q(\\theta , j)) $$ \n",
    "$$ \\triangledown f(\\theta) = \\frac{1}{k} \\sum_{j=1}^k \\triangledown Q(\\theta , j) $$\n",
    "$$ \\triangledown f(\\theta) = \\frac{1}{k} \\sum_{j=1}^k \\triangledown g(\\theta) = E[\\triangledown g(\\theta)] $$\n",
    "Hence Proved: \n",
    "$$ \\triangledown f(\\theta) = E[\\triangledown g(\\theta)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "## Setup:\n",
    "$$ lr = \\frac{1}{t} \\text{, } f(\\theta^*) = 0 $$\n",
    "$$ \\epsilon = |E f(\\theta _{t} ) - f(\\theta) | $$\n",
    "\n",
    "#### Note: e in the table stands for $\\epsilon$\n",
    "<br></br>\n",
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>SGD</th>\n",
    "    <th>GD</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Computational Cost</td>\n",
    "    <td>O(1)</td>\n",
    "    <td>O(k)</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Number of Updates </td>\n",
    "    <td>O(1/e)</td>\n",
    "    <td>O(log(1/e))</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>Total Cost</td>\n",
    "  <td>O(1/e)</td>\n",
    "  <td>O(k * log(1/e))</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "\n",
    "1. Gradient is only calculated only once thus 1.\n",
    "2. One gradient for each data point thus 1 * k = k.\n",
    "3. According to Theorem $ 1.1 $ the $ t = \\frac{1}{\\epsilon} $\n",
    "4. According to Theorem $ 1.1 $ and the assumption Assume finding ∇Q(θ, j) takes c time, and finding ∇f (θ) takes $ k * c $ time. \n",
    "5. Total Cost = Computational Cost * Number of Updates $ 1 * \\frac{1}{\\epsilon} $\n",
    "6. Total Cost = Computational Cost * Number of Updates $ k * \\log{(\\frac{1}{\\epsilon})} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "## Part 1.\n",
    "\n",
    "$$ \\rho = O(k^{-\\gamma}) \\text{;  } 0 < \\gamma <= 1 $$\n",
    "$$ \\epsilon = c * p \\text{; where c is a  +ve constant} $$ \n",
    "\n",
    "#### Note: p in the table stands for $\\rho$ and x refers to $\\rho ^ \\frac{-1}{\\gamma}$\n",
    "<br></br>\n",
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>SGD</th>\n",
    "    <th>GD</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Computational Cost</td>\n",
    "    <td>O(1)</td>\n",
    "    <td>O(x)</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> Number of Updates </td>\n",
    "    <td>O(1/p)</td>\n",
    "    <td>O(log(1/p))</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>Total Cost</td>\n",
    "  <td>O(1/p)</td>\n",
    "  <td>O(x * log(1/p))</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "1. Computational cost does not depend test precission.\n",
    "2. Computational cost does not depend test precission $ 1 * \\rho ^ \\frac{-1}{\\gamma} $.\n",
    "3. According to Theorem $ 1.1 $ the $ t = \\frac{1}{\\rho} $\n",
    "4. since $ \\epsilon = c * \\rho $ as number of updates is just scaled by a factor of $ c $ thus the number is just $ O(\\log{(\\frac{1}{\\rho})}) $. \n",
    "5. Total Cost = Computational Cost * Number of Updates $ 1 * \\frac{1}{\\rho} $\n",
    "6. Total Cost = Computational Cost * Number of Updates $  \\rho ^ \\frac{-1}{\\gamma} * \\log{(\\frac{1}{\\rho})} $\n",
    "\n",
    "\n",
    "## Part 2.\n",
    "As $ k $ gets bigger the cost for GD become ***significantly*** higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4        \n",
    "\n",
    "## Part 1.\n",
    "```py\n",
    "theta_pred = np.linalg.inv(A.T @ A) @ A.T @ y_data\n",
    "```\n",
    "$$ A_{\\theta} = y $$\n",
    "\n",
    "$$ \\theta = (A^T * A^{-1}) * A^T *y $$\n",
    "\n",
    "## Part 2.\n",
    "### HINT\n",
    "$$ r(θ) = h(e(θ)) = h(x^T_jθ − y_j ) $$\n",
    "$$ \\triangledown r(\\theta) = \\frac{\\partial h}{\\partial e}  \\triangledown e(\\theta) = \\frac{\\partial h}{\\partial e} *x_j$$\n",
    "\n",
    "### Setup\n",
    "$$ f(\\theta) = \\frac{1}{k} * \\sum_{j=1}^k Q(\\theta , j) $$\n",
    "$$  Q(\\theta , j) = |x^T_j \\theta - y_j |^\\gamma $$\n",
    "\n",
    "### Solution\n",
    "$$ f(\\theta) = \\frac{1}{k} * \\sum_{j=1}^k Q(\\theta , j) $$\n",
    "Taking log on both sides:\n",
    "$$ \\log{(Q(\\theta , j))} = \\gamma  \\log{|x^T_j \\theta -y_j|} $$\n",
    "$$ \\frac{1}{Q(\\theta , j)} \\triangledown Q(\\theta , j) = \\gamma \\frac{1}{|x^T_j \\theta -y_j|} \\frac{\\partial h}{\\partial e} x_j$$\n",
    "$$ \\triangledown Q(\\theta , j) =  \\gamma |x^T_j \\theta -y_j|^{\\gamma - 1} \\frac{\\partial h}{\\partial e} x_j \\text{  ---> Eq(1)}$$ \n",
    "$$ x_j \\frac{\\partial h}{\\partial e}  = s(x^T_j \\theta -y_j) x_j^T \\text{  ---> Eq(2)}$$\n",
    "Substituting Eq(2) in Eq(1)\n",
    "$$ \\triangledown Q(\\theta , j) = \\gamma |x^T_j \\theta -y_j|^{\\gamma - 1} s(x^T_j \\theta -y_j) x_j^T$$\n",
    "\n",
    "## Part 3.\n",
    "```python\n",
    "for method_idx, method in enumerate(['adam', 'sgd']):\n",
    "        test_loss_mat = []\n",
    "        train_loss_mat = []\n",
    "        \n",
    "        for replicate in range(num_rep):\n",
    "            if replicate % 20 == 0:\n",
    "                # print(method, replicate)\n",
    "\n",
    "                pass            \n",
    "            if method == 'adam':\n",
    "                # print('Adam Not implemented.')\n",
    "                beta_1 = 0.9\n",
    "                beta_2 = 0.999\n",
    "                m = 0\n",
    "                v = 0\n",
    "                epsilon = 10**-8\n",
    "\n",
    "            # if method == 'adagrad':\n",
    "            #     print('Adagrad Not implemented.')\n",
    "            #     epsilon = NotImplemented # TODO: Initialize parameters\n",
    "            #     squared_sum = NotImplemented\n",
    "                \n",
    "            theta_hat = theta_init.copy()\n",
    "            test_loss_list = []\n",
    "            train_loss_list = []\n",
    "\n",
    "            for t in range(max_iter):\n",
    "                idx = np.random.choice(data_num, batch_size) # Split data\n",
    "                train_loss, gradient = noisy_val_grad(theta_hat, A[idx,:], y_data[idx,:], deg_=deg_)\n",
    "                artificial_grad_noise = np.random.randn(10, 1) * grad_artificial_normal_noise_scale + np.sign(np.random.random((10, 1)) - 0.5) * 0.\n",
    "                gradient = gradient + artificial_grad_noise\n",
    "                train_loss_list.append(train_loss)\n",
    "                \n",
    "                if t % test_exp_interval == 0:\n",
    "                    test_loss, _ = noisy_val_grad(theta_hat, A_test[:,:], y_test[:,:], deg_=deg_)\n",
    "                    test_loss_list.append(test_loss)                \n",
    "                \n",
    "                if method == 'adam':\n",
    "                    # print('Adam Not implemented.') # TODO: Implement Adam\n",
    "                    # m = NotImplemented\n",
    "                    # v = NotImplemented\n",
    "                    # m_hat = NotImplemented\n",
    "                    # v_hat = NotImplemented\n",
    "                    # theta_hat = theta_hat - lr * NotImplemented\n",
    "                    m = beta_1 * m + (1 - beta_1) * gradient\n",
    "                    v = beta_2 * v + (1 - beta_2) * gradient**2\n",
    "                    m_hat = m / (1 - beta_1**(t + 1))\n",
    "                    v_hat = v / (1 - beta_2**(t + 1))\n",
    "                    theta_hat -=  lr * (m_hat / (np.sqrt(v_hat) + epsilon))\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                # elif method == 'adagrad':\n",
    "                #     print('Adagrad Not implemented.')\n",
    "                #     squared_sum = squared_sum + NotImplemented # TODO: Implement Adagrad\n",
    "                #     theta_hat = theta_hat - lr * NotImplemented\n",
    "                \n",
    "                elif method == 'sgd':\n",
    "                    theta_hat = theta_hat - lr * gradient\n",
    "                    \n",
    "            \n",
    "            test_loss_mat.append(test_loss_list)\n",
    "            train_loss_mat.append(train_loss_list)\n",
    "            \n",
    "        # print(method, 'done')\n",
    "        print(theta_hat)\n",
    "        best_vals[method] = min(test_loss_list)\n",
    "        x_axis = np.arange(max_iter)[::test_exp_interval]\n",
    "        \n",
    "        # print('test_loss_np is a 2d array with num_rep rows and each column denotes a specific update stage in training')\n",
    "        # print('The elements of test_loss_np are the test loss values computed in each replicate and training stage.')\n",
    "        test_loss_np = np.array(test_loss_mat)\n",
    "        \n",
    "        # print('Not implemented.')\n",
    "        '''\n",
    "        Hints:\n",
    "        1. Use test_loss_np in np.mean() with axis = 0\n",
    "        '''\n",
    "        test_loss_mean = np.mean(test_loss_np, axis=0) \n",
    "        '''\n",
    "        Hints: \n",
    "        1. Use test_loss_np in np.std() with axis = 0 \n",
    "        2. Divide by np.sqrt() using num_rep as a parameter\n",
    "        '''\n",
    "        test_loss_se = np.std(test_loss_np, axis=0) / np.sqrt(num_rep)\n",
    "        # plt.errorbar(x_axis, test_loss_mean, yerr=2.5*test_loss_se, label=method)\n",
    "        # best_vals[method] = min(test_loss_mean)\n",
    "        print('Best test loss for', method)\n",
    "\n",
    "```\n",
    "Best test loss for adam\n",
    "[[0.41802148]\n",
    " [0.24604471]\n",
    " [0.33396892]\n",
    " [0.30014015]\n",
    " [0.31440514]\n",
    " [0.36982198]\n",
    " [0.31147119]\n",
    " [0.31560022]\n",
    " [0.28837646]\n",
    " [0.26713612]]\n",
    " \n",
    "Best test loss for sgd\n",
    "[[0.57397516]\n",
    " [0.52989976]\n",
    " [0.52301592]\n",
    " [0.47828884]\n",
    " [0.49120472]\n",
    " [0.61891704]\n",
    " [0.45927503]\n",
    " [0.57437894]\n",
    " [0.50583345]\n",
    " [0.50121868]]\n",
    "\n",
    "\n",
    "## Part 4.\n",
    "\n",
    "<center>\n",
    "<a href=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihZikwfZ7oOP50n-NthQ2KJYeJLGW48I9dg3MYffS-ivMIUgf5W6TEFTSOVF2aQG9q1Y4pIRl9rPcfMOupp6e6bzgUBl5PbJaj4=s2560?source=screenshot.guru\"> <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihZikwfZ7oOP50n-NthQ2KJYeJLGW48I9dg3MYffS-ivMIUgf5W6TEFTSOVF2aQG9q1Y4pIRl9rPcfMOupp6e6bzgUBl5PbJaj4=s2560\" /> </a>\n",
    "</center>\n",
    "\n",
    "## Part 5.\n",
    "\n",
    "ADAM consistently outperforms SGD across multiple settings, yielding lower test losses. This superior performance of ADAM can be attributed to its adaptive learning rates and bias correction mechanisms, which allow it to handle varying gradient scales and achieve faster, more stable convergence compared to the constant learning rate approach of SGD.\n",
    "<center>\n",
    "<a href=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihYQSo6c3sbGvBsryqCRSgqYINOMakignLW0URz7y0DXfDPh-avQdX5xBVm-julHhsN1CPjzDeBghxBpSDKJ0lqxLZTErrYk8dI=s2560?source=screenshot.guru\"> <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihYQSo6c3sbGvBsryqCRSgqYINOMakignLW0URz7y0DXfDPh-avQdX5xBVm-julHhsN1CPjzDeBghxBpSDKJ0lqxLZTErrYk8dI=s2560\" /> </a>\n",
    "\n",
    "<a href=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihaGu7aTkm4KM9lkPYIzfgXKl2rY5wrl5cVVOUbQaF3L6wTr4qIGZWFiDoGVhLMFu251BqiA40BRqEacEkVSQ_FuQhc54kiDAs0=s2560?source=screenshot.guru\"> <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihaGu7aTkm4KM9lkPYIzfgXKl2rY5wrl5cVVOUbQaF3L6wTr4qIGZWFiDoGVhLMFu251BqiA40BRqEacEkVSQ_FuQhc54kiDAs0=s2560\" /> </a>\n",
    "\n",
    "<a href=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihY_LnvR4JX1FfAEG9vwAZFKvTtzrfhID9K9q-N69X39OAUcHJZwTsojEKb12EewWrglYpIHIVeOhrHFWs4SZ7Bjp5QZHObMBqQ=s2560?source=screenshot.guru\"> <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihY_LnvR4JX1FfAEG9vwAZFKvTtzrfhID9K9q-N69X39OAUcHJZwTsojEKb12EewWrglYpIHIVeOhrHFWs4SZ7Bjp5QZHObMBqQ=s2560\" /> </a>\n",
    "\n",
    "<a href=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihb42IzLrPFFezTE_sh48_K0AmO_Gm6vRo1lX71wnTc_RW2YFE7SWirw3RCgc0Uf66MXxMA-WAVDdkJuBNjvQUtoTf-6IZIHSQ=s2560?source=screenshot.guru\"> <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihb42IzLrPFFezTE_sh48_K0AmO_Gm6vRo1lX71wnTc_RW2YFE7SWirw3RCgc0Uf66MXxMA-WAVDdkJuBNjvQUtoTf-6IZIHSQ=s2560\" /> </a>\n",
    "\n",
    "<a href=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihbibvqMCZK6XU7RyYSe3_cnE5KTiA4I4Dxyc2sR8qjRg3VPRh2X-MR1_lvNuL2bYoY4850uTTd3Kn2moEsyF6gAfB6oe1-06MI=s2560?source=screenshot.guru\"> <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihbibvqMCZK6XU7RyYSe3_cnE5KTiA4I4Dxyc2sR8qjRg3VPRh2X-MR1_lvNuL2bYoY4850uTTd3Kn2moEsyF6gAfB6oe1-06MI=s2560\" /> </a>\n",
    "\n",
    "<a href=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihZPt4n5DUVyh_CNN3luG1k7hcLHDOOvwGXRqtTofZUP2bJ0Mbvh-jXmuFMSq2v85BT9b8h9y9AxCK7PO6zviDLeVpk7I57tiRA=s2560?source=screenshot.guru\"> <img src=\"https://lh3.googleusercontent.com/drive-viewer/AKGpihZPt4n5DUVyh_CNN3luG1k7hcLHDOOvwGXRqtTofZUP2bJ0Mbvh-jXmuFMSq2v85BT9b8h9y9AxCK7PO6zviDLeVpk7I57tiRA=s2560\" /> </a>\n",
    "\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "## Part 1.\n",
    "1. There whole dataset is being used which basically turns it into a batch gradient descent (GD) algorithm. This is because SGD's characteristic of making updates based on a small subset of the data is negated, and instead, it uses the full dataset to compute the gradient and make updates, just like standard GD. Training Time (in minutes) = 1.4485693415006002\n",
    "2. The accuracy is not very high. The accuracy is only somewhere around 12.45% which is not great.\n",
    "3. Decreasing the size of dataset and decreasing the learning rate (using SGD) and increasing the number epochs can also be very beneficial.\n",
    "\n",
    "\n",
    "## Part 2. \n",
    "1. The accuracy is extremely high it was a faster method (Training Time (in minutes) = 1.8652761658032735). Accuracy was 92.29%.\n",
    "2. Data Augmentation: Increase the dataset's variety by applying transformations such as rotation, scaling, or translation to the images, helping the model generalize better.\n",
    "\n",
    "## Part 3.\n",
    "1. accuracy was 73.39% it was faster than both GD and SGD.\n",
    "2. ADAM is an adaptive learning rate optimizer that often converges faster than SGD due to its consideration of the first and second moments of the gradients. You might observe a more rapid decrease in the loss function and potentially higher accuracy from the start. ADAM converges faster than SGD because it adapts the learning rate for each weight in the network individually, based on estimates of the first and second moments of the gradients. This allows it to take larger steps for weights associated with features that have consistent gradients and smaller steps where the gradients are more variable.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
