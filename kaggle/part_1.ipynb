{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "dim_theta = 10\n",
    "data_num = 1000\n",
    "scale = .1\n",
    "\n",
    "A = np.random.uniform(low=-1.0, high=1.0, size=(data_num,dim_theta))\n",
    "y_data = np.matmul(A,theta_true)+np.random.normal(loc=0.0, scale=scale, size=(data_num,1))\n",
    "\n",
    "A_test = np.random.uniform(low=-1.0, high=1.0, size=(50,dim_theta))\n",
    "y_test = np.matmul(A_test,theta_true)+np.random.normal(loc=0.0, scale=scale, size=(50,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the exact solution for the mean squared loss (solving Ax = b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_true = np.ones((dim_theta,1))\n",
    "print('True theta:', theta_true.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Not implemented. Student has to implement this part and create theta_pred using A and y_data from above')\n",
    "theta_pred = ????\n",
    "print('Empirical theta', theta_pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Variants Noisy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "max_iter = 1000\n",
    "lr = 0.001\n",
    "theta_init = np.random.random((10,1)) * 0.1\n",
    "theta_hat = np.ones((10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy function that can be used in place of noisy_val_grad for the SGD variants (optional)\n",
    "# def noisy_mse_val_grad(theta_hat, data_, label_, deg_=2.):\n",
    "    # Atheta_minus_y_idx=np.matmul(data_,theta_hat) - label_\n",
    "    # gradient = 2*np.matmul(np.transpose(data_), Atheta_minus_y_idx)\n",
    "    # loss = np.sum(Atheta_minus_y_idx**2)\n",
    "    # return loss / data_.shape[0],gradient / data_.shape[0]\n",
    "\n",
    "def noisy_val_grad(theta_hat, data_, label_, deg_=2.):\n",
    "    gradient = np.zeros_like(theta_hat)\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(data_.shape[0]):\n",
    "        x_ = data_[i, :].reshape(-1,1)\n",
    "        y_ = label_[i, 0]\n",
    "        err = np.sum(x_ * theta_hat) - y_\n",
    "        \n",
    "        print('Not implemented. Student has to implement this part and create l_pt and grad_pt')\n",
    "        grad = ????\n",
    "        l = ????\n",
    "        \n",
    "        loss += l / data_.shape[0]\n",
    "        gradient += grad / data_.shape[0]\n",
    "        \n",
    "    return loss, gradient\n",
    "\n",
    "noisy_val_grad = noisy_poly_val_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SGD Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_ = 2.\n",
    "num_rep = 10\n",
    "max_iter = 1000\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "best_vals = dict()\n",
    "test_exp_interval = 50\n",
    "grad_artificial_normal_noise_scale = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_idx, method in enumerate(['adam', 'sgd', 'adagrad']):\n",
    "    test_loss_mat = []\n",
    "    train_loss_mat = []\n",
    "    \n",
    "    for replicate in range(num_rep):\n",
    "        if replicate % 20 == 0:\n",
    "            print(method, replicate)\n",
    "            \n",
    "        if method == 'adam':\n",
    "            print('Not implemented. Student has to initialize and create necessary hyperparameters/moments of ADAM such as beta_1, m, nu, etc.')\n",
    "            ?????\n",
    "        if method == 'adagrad':\n",
    "            print('Not implemented. Student has to initialize and create necessary hyperparameters/moments of ADAGRAD ')\n",
    "            ??????\n",
    "            \n",
    "#       Optional\n",
    "#         if method == 'rmsprop':\n",
    "#             print('This is not mandatory. Implement it for your own fun.')\n",
    "#             print('Not implemented. Student has to initialize and create necessary hyperparameters/moments of RMSPROP')\n",
    "#             ?????\n",
    "            \n",
    "        theta_hat = theta_init.copy()\n",
    "        test_loss_list = []\n",
    "        train_loss_list = []\n",
    "        \n",
    "        for t in range(max_iter):\n",
    "            idx = np.random.choice(data_num,batch_size)\n",
    "            train_loss, gradient = noisy_val_grad(theta_hat, A[idx,:], y_data[idx,:], deg_=deg_)\n",
    "            artificial_grad_noise = np.random.randn(10,1) * grad_artificial_normal_noise_scale + np.sign(np.random.random((10,1))-0.5) * 0.\n",
    "            gradient = gradient + artificial_grad_noise\n",
    "            train_loss_list.append(train_loss)\n",
    "            \n",
    "            if t % test_exp_interval == 0:\n",
    "                test_loss, _ = noisy_val_grad(theta_hat, A_test[:,:], y_test[:,:], deg_=deg_)\n",
    "                test_loss_list.append(test_loss)                \n",
    "            \n",
    "            if method == 'adam':\n",
    "                print('Not implemented. Student has to implement the core adam algorithm.')\n",
    "                ????\n",
    "                theta = theta - lr * ????\n",
    "            elif method == 'adagrad':\n",
    "                print('Not implemented. Student has to implement the core adagrad algorithm')\n",
    "                ????\n",
    "                theta = theta - lr * ????\n",
    "            elif method == 'sgd':\n",
    "                theta_hat = theta_hat - lr * gradient\n",
    "                \n",
    "#           Optional\n",
    "#             elif method=='rmsprop':\n",
    "#                 print('Not implemented. Student has to implement the core rmsprop algorithm (this is optional)')\n",
    "#                 ????\n",
    "#                 theta = theta - lr * ????\n",
    "        \n",
    "        test_loss_mat.append(test_loss_list)\n",
    "        train_loss_mat.append(train_loss_list)\n",
    "        \n",
    "    print(method, 'done')\n",
    "    x_axis = np.arange(max_iter)[::test_exp_interval]\n",
    "    \n",
    "    test_loss_np = np.array(test_loss_mat)\n",
    "    \n",
    "    print('Not implemented. Student has to create the test loss mean and the unbiased standard error vectors.')\n",
    "    print('test_loss_np is a 2d array with num_rep rows and some number of columns, where each column denotes a specific update stage in training.')\n",
    "    print('The elements of test_loss_np are the test loss values computed in each replicate and training stage.')\n",
    "    test_loss_mean = ????\n",
    "    test_loss_se = ????\n",
    "\n",
    "    plt.errorbar(x_axis, test_loss_mean, yerr=2.5*test_loss_se, label=method)\n",
    "    best_vals[method] = min(test_loss_mean)\n",
    "    \n",
    "best_vals = {k: int(v*1000)/1000. for k,v in best_vals.items()} # A weird way to round numbers\n",
    "plt.title(f'Test Loss \\n(objective degree: {deg_},  best values: {best_vals})')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Updates')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
